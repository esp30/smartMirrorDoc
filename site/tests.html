---
layout: page
title: Testing
description: "How we assess our system works"
button: false
button-ref: "/assets/docs/QA.pdf"
button-description: "QA Report + Risk Assessment"
header-image: true
header-path: "images/tests.jpg"
toc: true
---

<div id="how">
    <div class="title" style="text-align: center">
        <h2>Specification</h2>
        <h3 data-toc-skip>Personas and Scenarions</h3>
        Based on a User Centered Design perspective, we defined the key personas and usage scenarios to define the acceptance criteria of our system. These criteria were the support for most of implemented tests, as described in the next section.
    </div>

</div>

<div id="how">
    <div class="title" style="text-align: center">
        <h3>Persona</h3>
        <h4 data-toc-skip>Who will use our system and why</h4>
    </div>

    <div class="card-deck">
        {% for persona in site.data.personas %}
        <div class="card" style="width: 20rem;">
            <a>
                <img class="card-img-top" style="width: 100%;" src="{{ site.assets }}/images/persona/{{ persona.photo }}" alt="{{ persona.name }}">
            </a>

            <div class="card-header card-header-primary">
                <a style="font-weight: bold">{{persona.name}}</a>
                <p class="category">{{persona.description}}</p>
            </div>
            <ul class="list-group list-group-flush">
                    <li class="list-group-item" style="font-weight: bold">Motivations:</li>
                {% for characteristic in persona.characteristics %} 
                    <li class="list-group-item">{{characteristic}}</li>
                {% endfor %}
            </ul>
        </div>
        {% endfor %}
    </div>

<div id="how">
<div class="title" style="text-align: center">
    <h3>Usage Scenarios</h2>
    <h4 data-toc-skip>How the system will be used</h4>
</div>

<div class="list-group">
        <!--Table-->
        <table id="tablePreview" class="table" style="table-layout:fixed">
            <!--Table head-->
            <thead>
            <tr>
                <th>Feature</th>
                <th>Scenario</th>
                <th>Scenario Description</th>
                <th>Scenario Status</th>
            </tr>
            </thead>
            
            <tbody>
                {% for feature in site.data.scenarios %}
                {% if feature.FEATURE-STATUS == ToDo %}
                <tr>
                    <th scope="row">{{ feature.FEATURE_TITLE }}<br>{{ feature.FEATURE_DESCRIPTION }}</th>
                    <td>{{ feature.SCENARIO }} </td>
                    <td>{{ feature.SCENARIO_DESCRIPTION }}</td>
                    <td>{{ feature.SCENARIO_STATUS }}</td>
                </tr>
                {% endif %}
                {% endfor %}
            </tbody>
        </table>
        
        
    </div>
    

<div id="acceptance-tests">
    <div class="title" style="text-align: center">
        <h2>Implementation</h2>
    </div>

    From the defined features, we chose to implement the first two (Report Status and Remotely Check Patient Status). <br><br>

    Having in mind a <b>Behavior-driven development</b> (BDD) mindset, the defined scenarios for each feature were the basis for the testing of the system. <i>Post-production tests based on those scenarios were implemented using the Cucumber tool.</i><br><br>

    <b>To make the testing process more robust and also quicker</b> during the execution of the DevOps pipeline, we opted to <b>divide the tests</b> implemented using the Cucumber tool <b>into 2 sets: online and offline tests</b>. The difference between these 2 types lies on the usage or not of the external API the system provides. This division was useful to make sure that, if needed, only part of the tests can be run when needed (and not all of them) and also to allow parallelization of the execution of the 2 sets of tests in the pipeline (see the Cucumber Tests stages <a href="">here</a>). <br><br>

    It was also developed an <b>integration test</b> that uses the API to retrieve the emotion counts (statistics), then it injects a new image in the system, simulating a photo taken by the mirror. This image is processed by the emotion detection module that adds the new emotion to the system. The last step of this test requires a new request to the same end point of the API which will allow for a comparison between the first emotion counts and the updated one. The difference between them should correspond to the emotion present in the image injected. <br><br>

    <b>Tests can be run through the execution of the Jenkins pipeline</b> of the project, where it's possible to see the reports prepared in the last stage of the pipeline as illustrated below (<i>Cucumber reports Jenkins plugin</i> is required). <br><br>

    <img class="center" src="{{site.assets}}/images/cucumber_example.png" alt="Project Architecture Diagram" style="max-width:100%;
    max-height:100%;"></img>
    <p class="text-muted" style="text-align: center">
        Reports for feature 2 (Remotely Check Patient Status) as seen in the Jenkins dashboard
    </p>

    <br><br>
   
    <b>Alternatively</b>, the tests can be run using <code>mvn test -Dcucumber.options="--tags @online/@offline --tags ~@not-implemented" -s settings.xml</code> if you only want to run online/offline tests or <code>mvn test -Dcucumber.options= --tags ~@not-implemented" -s settings.xml</code> if you want to run all the tests. In these situations, the results will be available in the file <code>target/cucumber/report.json</code>

</div>

<br>


